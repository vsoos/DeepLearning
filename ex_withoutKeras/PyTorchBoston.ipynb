{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Extra Exercise Project - PyTorch for linear Regression\n",
        "\n",
        "https://www.kaggle.com/code/simpleparadox/boston-housing-dataset-pytorch"
      ],
      "metadata": {
        "id": "CgBSYBNwlbV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC1mgJZRKoSq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/deeplearning2024_VincenzinaSoos/ex_withoutKeras\"\n",
        "os.chdir(folder_path)"
      ],
      "metadata": {
        "id": "eNYIgk5TMZKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"data/boston-housing-dataset.csv\")"
      ],
      "metadata": {
        "id": "QY2JUkFCKsCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3Uao9N2wKu1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = StandardScaler().fit_transform(dataset)"
      ],
      "metadata": {
        "id": "ugyj-N0xKwkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model and define RMSE loss.\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.linear = nn.Linear(13, 1)  # Define the weight matrix. Size: Number of features x Number of targets\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(F.mse_loss(yhat, y))"
      ],
      "metadata": {
        "id": "aMGeq6dkKyHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(num_epochs, model, loss_fn, optimizer, train_dataloader, val_loader):\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, targets in train_dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            # Get predictions.\n",
        "            preds = model(inputs)\n",
        "\n",
        "            # Get loss.\n",
        "            loss = loss_fn(preds, targets)\n",
        "\n",
        "            # Compute gradients.\n",
        "            loss.backward()\n",
        "            #print(loss.item())\n",
        "\n",
        "            # Update model parameters i.e. backpropagation.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Reset gradients to zero before the next epoch.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            # Get validation loss as well.\n",
        "            for val_input, val_targets in val_loader:\n",
        "                val_input, val_targets = val_input.to(device), val_targets.to(device)\n",
        "                out = model(val_input)\n",
        "                val_loss = RMSELoss(out, val_targets)\n",
        "            print(\"Epoch [{}/{}], Training loss: {:.4f}, Validation Loss: {:.4f}\".format(epoch + 1, num_epochs, loss.item(), val_loss)) # Report loss value after each epoch."
      ],
      "metadata": {
        "id": "xoyCrv0hKzhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model training metadata.\n",
        "num_epochs = 2000\n",
        "model = LinearRegression()\n",
        "model.to(device)\n",
        "loss_fn = RMSELoss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
        "\n",
        "# Making sure shapes are correct and defining dataloader.\n",
        "input_np_array = np.array(dataset[:,1:-1].tolist(), dtype='float32')\n",
        "inputs = torch.from_numpy(input_np_array)\n",
        "# inputs = torch.from_numpy(np.array(dataset.iloc[:,1:-1].values, dtype='float32'))\n",
        "print(inputs.shape)\n",
        "targets_np_array = np.array(dataset[:,-1].tolist(), dtype='float32')\n",
        "targets_np_array_rs = np.reshape(targets_np_array, (targets_np_array.shape[0],1))\n",
        "targets = torch.from_numpy(targets_np_array_rs)\n",
        "print(targets.shape)\n",
        "# targets = torch.from_numpy(np.array(dataset.iloc[:,-1].values, dtype='float32'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocAnIWzfK3ph",
        "outputId": "9cb094b8-0469-4b57-f7c9-390951d74b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([506, 13])\n",
            "torch.Size([506, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tensor_dataset = TensorDataset(inputs, targets)\n",
        "val_size, train_size = int(0.1 * len(train_tensor_dataset)),  len(train_tensor_dataset) - int(0.1 * len(train_tensor_dataset))\n",
        "batch_size = 50\n",
        "train_data, val_data = random_split(train_tensor_dataset, [train_size, val_size])\n",
        "train_dataloader = DataLoader(train_data, batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size)\n",
        "\n",
        "# # Train the model.\n",
        "fit(num_epochs, model, loss_fn, optimizer, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkSLxrY8Lxf7",
        "outputId": "72af7982-5c83-4d16-f929-ce9a9748af48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/2000], Training loss: 0.3042, Validation Loss: 0.7481\n",
            "Epoch [100/2000], Training loss: 1.0054, Validation Loss: 0.6731\n",
            "Epoch [150/2000], Training loss: 0.3865, Validation Loss: 0.6562\n",
            "Epoch [200/2000], Training loss: 0.2688, Validation Loss: 0.6508\n",
            "Epoch [250/2000], Training loss: 0.3153, Validation Loss: 0.6483\n",
            "Epoch [300/2000], Training loss: 0.4052, Validation Loss: 0.6440\n",
            "Epoch [350/2000], Training loss: 0.3579, Validation Loss: 0.6413\n",
            "Epoch [400/2000], Training loss: 0.3586, Validation Loss: 0.6388\n",
            "Epoch [450/2000], Training loss: 0.4308, Validation Loss: 0.6357\n",
            "Epoch [500/2000], Training loss: 1.2165, Validation Loss: 0.6356\n",
            "Epoch [550/2000], Training loss: 0.6808, Validation Loss: 0.6335\n",
            "Epoch [600/2000], Training loss: 0.3497, Validation Loss: 0.6325\n",
            "Epoch [650/2000], Training loss: 0.4208, Validation Loss: 0.6309\n",
            "Epoch [700/2000], Training loss: 0.2391, Validation Loss: 0.6302\n",
            "Epoch [750/2000], Training loss: 0.7644, Validation Loss: 0.6306\n",
            "Epoch [800/2000], Training loss: 0.3183, Validation Loss: 0.6303\n",
            "Epoch [850/2000], Training loss: 1.1547, Validation Loss: 0.6294\n",
            "Epoch [900/2000], Training loss: 0.4125, Validation Loss: 0.6297\n",
            "Epoch [950/2000], Training loss: 0.2740, Validation Loss: 0.6300\n",
            "Epoch [1000/2000], Training loss: 0.3321, Validation Loss: 0.6278\n",
            "Epoch [1050/2000], Training loss: 0.2261, Validation Loss: 0.6279\n",
            "Epoch [1100/2000], Training loss: 0.3378, Validation Loss: 0.6288\n",
            "Epoch [1150/2000], Training loss: 0.7644, Validation Loss: 0.6273\n",
            "Epoch [1200/2000], Training loss: 0.2947, Validation Loss: 0.6266\n",
            "Epoch [1250/2000], Training loss: 0.2334, Validation Loss: 0.6239\n",
            "Epoch [1300/2000], Training loss: 0.4873, Validation Loss: 0.6251\n",
            "Epoch [1350/2000], Training loss: 0.5797, Validation Loss: 0.6260\n",
            "Epoch [1400/2000], Training loss: 0.3241, Validation Loss: 0.6255\n",
            "Epoch [1450/2000], Training loss: 0.2384, Validation Loss: 0.6253\n",
            "Epoch [1500/2000], Training loss: 0.3203, Validation Loss: 0.6253\n",
            "Epoch [1550/2000], Training loss: 0.7017, Validation Loss: 0.6237\n",
            "Epoch [1600/2000], Training loss: 0.4253, Validation Loss: 0.6238\n",
            "Epoch [1650/2000], Training loss: 0.9059, Validation Loss: 0.6246\n",
            "Epoch [1700/2000], Training loss: 0.2168, Validation Loss: 0.6273\n",
            "Epoch [1750/2000], Training loss: 0.4806, Validation Loss: 0.6259\n",
            "Epoch [1800/2000], Training loss: 0.4429, Validation Loss: 0.6253\n",
            "Epoch [1850/2000], Training loss: 0.5178, Validation Loss: 0.6247\n",
            "Epoch [1900/2000], Training loss: 0.3351, Validation Loss: 0.6246\n",
            "Epoch [1950/2000], Training loss: 0.5395, Validation Loss: 0.6263\n",
            "Epoch [2000/2000], Training loss: 0.4278, Validation Loss: 0.6236\n"
          ]
        }
      ]
    }
  ]
}