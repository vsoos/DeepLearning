{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Exercise Project Manual Neural Networks 1\n",
        "\n",
        "I found this notebook online earlier in the semester, but unfortunately I didn’t save the link. I still wanted to include it because I learned a lot from going through it.\n",
        "\n",
        "This notebook contains a simple neural network with two layers:\n",
        "\n",
        "- The first layer has 4 inputs and 2 neurons.\n",
        "\n",
        "- The second layer takes those 2 values and passes them to 3 output neurons.\n",
        "\n",
        "ReLU activation is used after the first layer and SoftMax at the end to get class probabilities.\n",
        "\n",
        "Then the code uses multiple loss functions, namely:\n",
        "\n",
        "- Categorical cross-entropy\n",
        "\n",
        "- Binary cross-entropy\n",
        "\n",
        "- KL divergence\n",
        "\n",
        "- Hinge loss\n",
        "\n",
        "Each one was calculated manually using NumPy,which is very impressive.\n",
        "\n",
        "\n",
        "I found this useful for comparing different ways of measuring how wrong the model’s predictions were. For example, the categorical crossentropy loss was about 0.33, and the binary cross-entropy was lower, around 0.19. These made me understand how the models accuracy can change depending on the task and on the loss function used.\n",
        "\n",
        "What I learned from this notebook is how neural network layers are structured and how they transform input into output. I also noticed that the SoftMax layer was very sensitive to small changes in weights or inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "cy9eYQr-ExbP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4t1ZwJTgHpy",
        "outputId": "a4346920-85a3-46d9-b725-1d2f8b21f366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Output Probabilities (SoftMax):\n",
            " [[0.3330081  0.33231371 0.33467819]\n",
            " [0.3328193  0.33172567 0.33545502]\n",
            " [0.33262964 0.3311377  0.33623267]]\n",
            "Categorical Cross-Entropy Loss: -0.3317256915856903\n",
            "Binary Cross-Entropy Loss: 0.19763488164214868\n",
            "KL Divergence: 0.030478754035472025\n",
            "Hinge Loss: 0.5\n"
          ]
        }
      ],
      "source": [
        "# Seed for reproducibility\n",
        "np.random.seed(10)\n",
        "\n",
        "# Input data\n",
        "X = [[1.0, 2.0, 3.0, 4.0], [2.0, 3.0, 4.0, 5.0], [3.0, 4.0, 5.0, 6.0]]\n",
        "\n",
        "# Define a dense layer class\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        # Randomly initialize weights, scaled to 0.10\n",
        "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
        "        # Initialize biases as zeros\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Forward pass: calculate weighted sum (Z = XW + B)\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# Define the ReLU activation function\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        # ReLU: Output is max(0, input)\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "# Define the SoftMax activation function\n",
        "class Activation_SoftMax:\n",
        "    def forward(self, inputs):\n",
        "        # Stabilize inputs to avoid large exponentials\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=-1, keepdims=True))\n",
        "        # Normalize by sum of exponentials\n",
        "        self.output = exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n",
        "\n",
        "# Define a categorical cross-entropy loss function\n",
        "class Loss_CategoricalCrossEntropy:\n",
        "    def forward(self, predicted_prob, true_labels):\n",
        "        epsilon = 1e-15  # Small constant to avoid log(0)\n",
        "        # Clip predicted probabilities to avoid log(0) or log(1)\n",
        "        clipped = np.clip(predicted_prob, epsilon, 1 - epsilon)\n",
        "        # Calculate loss based on true labels\n",
        "        if len(true_labels.shape) == 1:\n",
        "            # If labels are not one-hot encoded, use indices\n",
        "            loss = -np.log(clipped[range(len(predicted_prob)), true_labels])\n",
        "        elif len(true_labels.shape) == 2:\n",
        "            # If labels are one-hot encoded\n",
        "            loss = -np.sum(clipped * true_labels, axis=1)\n",
        "        # Return the mean loss\n",
        "        return np.mean(loss)\n",
        "\n",
        "# Step 1: Define layers\n",
        "layer1 = Layer_Dense(4, 2)  # First layer with 4 inputs and 2 neurons\n",
        "activation1 = Activation_ReLU()  # Activation function for the first layer\n",
        "\n",
        "layer2 = Layer_Dense(2, 3)  # Second layer with 2 inputs and 3 neurons\n",
        "activation2 = Activation_SoftMax()  # Activation function for the second layer\n",
        "\n",
        "# Step 2: Forward propagation\n",
        "# Forward pass through first layer and activation\n",
        "layer1.forward(X)\n",
        "activation1.forward(layer1.output)\n",
        "\n",
        "# Forward pass through second layer and activation\n",
        "layer2.forward(activation1.output)\n",
        "activation2.forward(layer2.output)\n",
        "\n",
        "# Print outputs after second activation\n",
        "print(\"Final Output Probabilities (SoftMax):\\n\", activation2.output)\n",
        "\n",
        "# Step 3: Loss calculation\n",
        "# Example true labels (one-hot encoded)\n",
        "true_labels = np.array([[0, 1, 0], [0, 1, 0], [0, 1, 0]])\n",
        "\n",
        "# Initialize loss function\n",
        "loss_function = Loss_CategoricalCrossEntropy()\n",
        "# Calculate loss\n",
        "loss = loss_function.forward(activation2.output, true_labels)\n",
        "print(\"Categorical Cross-Entropy Loss:\", loss)\n",
        "\n",
        "# Additional: Binary Cross-Entropy Loss Function\n",
        "def binary_cross_entropy(y, y_hat):\n",
        "    epsilon = 1e-15  # Small constant to avoid log(0)\n",
        "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)  # Clip predicted probabilities\n",
        "    loss = - (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))  # BCE formula\n",
        "    return np.mean(loss)  # Return mean loss\n",
        "\n",
        "# Example for binary classification\n",
        "y_binary = np.array([1, 0, 1, 0])  # True labels\n",
        "y_hat_binary = np.array([0.9, 0.1, 0.8, 0.3])  # Predicted probabilities\n",
        "binary_loss = binary_cross_entropy(y_binary, y_hat_binary)\n",
        "print(\"Binary Cross-Entropy Loss:\", binary_loss)\n",
        "\n",
        "# Additional: KL Divergence\n",
        "def kl_divergence(p, q):\n",
        "    epsilon = 1e-15\n",
        "    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "    q = np.clip(q, epsilon, 1 - epsilon)\n",
        "    return np.sum(p * np.log(p / q))\n",
        "\n",
        "# Example distributions\n",
        "p_distribution = np.array([0.2, 0.5, 0.3])\n",
        "q_distribution = np.array([0.3, 0.4, 0.3])\n",
        "kl_loss = kl_divergence(p_distribution, q_distribution)\n",
        "print(\"KL Divergence:\", kl_loss)\n",
        "\n",
        "# Additional: Hinge Loss\n",
        "def hinge_loss(y_true, raw_output):\n",
        "    return max(0, 1 - y_true * raw_output)\n",
        "\n",
        "# Example for hinge loss\n",
        "y_true_hinge = 1\n",
        "raw_output_hinge = 0.5\n",
        "hinge_loss_value = hinge_loss(y_true_hinge, raw_output_hinge)\n",
        "print(\"Hinge Loss:\", hinge_loss_value)\n"
      ]
    }
  ]
}