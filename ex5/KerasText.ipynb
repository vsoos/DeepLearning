{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise project 5 - Transformer networks\n",
        "\n",
        "The goal of this notebook was to implement a Transformer-based text classification model for sentiment analysis. The model architecture was from the Keras example on text classification using Transformer layers, which demonstrated how to efficiently process and classify text data.\n",
        "\n",
        "The dataset consisted of 50,000 movie reviews split equally between training and validation sets. The reviews were preprocessed to keep only the top 20,000 most frequent words, and each review was truncated or padded to a length of 200 words.\n",
        "\n",
        "The data was prepared with a vocabulary size of 20,000 and a maximum sequence length of 200. Padding was applied to make all sequences the same length.\n",
        "\n",
        "\n",
        "https://keras.io/examples/nlp/text_classification_with_transformer/\n"
      ],
      "metadata": {
        "id": "Wmcr5vRV2x_x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlL5zbHXEA5A"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a Transformer block as a laye"
      ],
      "metadata": {
        "id": "uGANOGyjEftz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "IJwKctU8EE3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement embedding layer\n",
        "Two separate embedding layers, one for tokens, one for token index (positions)."
      ],
      "metadata": {
        "id": "rdy0_hPcEaAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "rfDlirlAEHmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and prepare dataset"
      ],
      "metadata": {
        "id": "SvLOInr3EXud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsSdmesVEJLJ",
        "outputId": "5049ca72-64df-4ed5-ff71-5bc60071eede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create classifier model using transformer layer\n",
        "Transformer layer outputs one vector for each time step of our input sequence. Here, we take the mean across all time steps and use a feed forward network on top of it to classify text."
      ],
      "metadata": {
        "id": "hf3y4J8cERSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "lH2VsAXxEK37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model architecture is a simplified version of the Transformer, focusing on text classification instead of language generation or translation.\n"
      ],
      "metadata": {
        "id": "eUyZrYkb3IM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate\n",
        "\n"
      ],
      "metadata": {
        "id": "ze6huFrVEMlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The model was trained using the Adam optimizer with Sparse Categorical Cross-Entropy as the loss function since the labels were integers, not one-hot encoded. Training was done for 2 epochs with a batch size of 32.\n"
      ],
      "metadata": {
        "id": "o0ujsUo83LRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_jrpFBrEMNo",
        "outputId": "90b12247-a829-482b-cea0-d9bcb0983f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 147ms/step - accuracy: 0.7160 - loss: 0.5180 - val_accuracy: 0.8796 - val_loss: 0.2856\n",
            "Epoch 2/2\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 146ms/step - accuracy: 0.9266 - loss: 0.1965 - val_accuracy: 0.8704 - val_loss: 0.3271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Personal Analysis / Refelection"
      ],
      "metadata": {
        "id": "bk4yMIvhfgr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieved an accuracy of 92.66% on the training set and 87.04% on the validation set, which I found to be impressive, considering the model was trained for only 2 epochs. However, I noticed that the training loss continued to decrease while the validation loss increased, indicating overfitting.\n",
        "\n",
        "The Transformer block captured relationships well, even in long sequences of 200 words. The Embedding Layer with positional embeddings helped provide context. Despite training for only 2 epochs, the model performed well, showing it learned the dataset, patterns effectively.\n"
      ],
      "metadata": {
        "id": "llpuOerZ3X4V"
      }
    }
  ]
}